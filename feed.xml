

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://c0ldstudy.github.io/</id>
  <title>c0ldstudy</title>
  <subtitle>A minimal, responsive and feature-rich Jekyll theme for technical writing.</subtitle>
  <updated>2023-06-27T19:18:38-07:00</updated>
  <author>
    <name>Jiacen (Jason) Xu</name>
    <uri>http://c0ldstudy.github.io/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://c0ldstudy.github.io/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://c0ldstudy.github.io/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator>
  <rights> Â© 2023 Jiacen (Jason) Xu </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>LLM Paper Summary</title>
    <link href="http://c0ldstudy.github.io/posts/LLM_Paper_Summary/" rel="alternate" type="text/html" title="LLM Paper Summary" />
    <published>2023-06-10T00:00:00-07:00</published>
  
    <updated>2023-06-10T00:00:00-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/LLM_Paper_Summary/</id>
    <content src="http://c0ldstudy.github.io/posts/LLM_Paper_Summary/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="NLP" />
    
  

  
    <summary>
      





      1. Attention is All You Need
Paper/TF_code/Torch_code

Main Idea
The paper is the first to propose a parallel Transformer in the sequence transduction models.

Key insight
The problems of the RNN (LSTM, GRU) model: Need to calculate $h_t$ by $h_{t-1}$ which cannot be executed in parallel. While Transformer reduces the sequential computation

Model Structure:



  Position Encoding: they use sin...
    </summary>
  

  </entry>

  
  <entry>
    <title>Transformer Related Implementation</title>
    <link href="http://c0ldstudy.github.io/posts/Transformer_Related_Implementation/" rel="alternate" type="text/html" title="Transformer Related Implementation" />
    <published>2023-05-20T00:00:00-07:00</published>
  
    <updated>2023-05-20T00:00:00-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/Transformer_Related_Implementation/</id>
    <content src="http://c0ldstudy.github.io/posts/Transformer_Related_Implementation/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="NLP" />
    
  

  
    <summary>
      





      1. Transformer[1]

I start it by following the video[1] from Andrej Karpathy and check the code from the repo. In the video, he shows how to build a GPT by oneself step by step which also shows the way to build a NLP model in a high level. I summarize these steps and add some useful tips that mentioned in the video.

1.1 Data Preprocessing
Tokenizer

1.2 Bigram Model
The code is here.
High-leve...
    </summary>
  

  </entry>

  
  <entry>
    <title>ChatGPT Prompt Course Note</title>
    <link href="http://c0ldstudy.github.io/posts/ChatGPT_Prompt/" rel="alternate" type="text/html" title="ChatGPT Prompt Course Note" />
    <published>2023-04-25T00:00:00-07:00</published>
  
    <updated>2023-05-08T21:20:38-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/ChatGPT_Prompt/</id>
    <content src="http://c0ldstudy.github.io/posts/ChatGPT_Prompt/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="NLP" />
    
  

  
    <summary>
      





      The note is for the course: DeepLearning.AI ChatGPT Prompt Engineering Course.

ChatGPT Prompt Engineering for Developers
Course link: https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction
1 Basic concepts:

  Base LLM: Predict the next word based on the training data.
  Instruction Tuned LLM: Fine-tune on instructions and good attempts at following instructions.


2 Prompting ...
    </summary>
  

  </entry>

  
  <entry>
    <title>[DSN2023] On Adversarial Robustness of Point Cloud Semantic Segmentation</title>
    <link href="http://c0ldstudy.github.io/posts/PC_attack/" rel="alternate" type="text/html" title="[DSN2023] On Adversarial Robustness of Point Cloud Semantic Segmentation" />
    <published>2023-04-17T00:00:00-07:00</published>
  
    <updated>2023-04-21T11:06:49-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/PC_attack/</id>
    <content src="http://c0ldstudy.github.io/posts/PC_attack/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Papers" />
    
    <category term="Presentation" />
    
  

  
    <summary>
      





      Title: On Adversarial Robustness of Point Cloud Semantic Segmentation

  Author: Jiacen Xu, Zhe Zhou, Boyuan Feng, Yufei Ding and Zhou Li

  Conference: The 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Network, June, 2023.

  Paper: link

  Code: https://github.com/C0ldstudy/PointSecGuard

  Bibtex: Coming soon.


Main Idea
We present a comparative study of PCSS robu...
    </summary>
  

  </entry>

  
  <entry>
    <title>Reinforcement Learning Course from Hugging Face</title>
    <link href="http://c0ldstudy.github.io/posts/Reinforcement_Learning_Course/" rel="alternate" type="text/html" title="Reinforcement Learning Course from Hugging Face" />
    <published>2023-03-11T00:00:00-08:00</published>
  
    <updated>2023-06-10T09:51:07-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/Reinforcement_Learning_Course/</id>
    <content src="http://c0ldstudy.github.io/posts/Reinforcement_Learning_Course/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="RL" />
    
  

  
    <summary>
      





      I use the blog to record my learning procedures of reinforcement learning course from Hugging Face

Unit 1: Introduction to Deep Reinforcement Learning

Concepts:

  Reinforcement Learning is a computational approach of learning from action. An agent is designed based on the environment interactions with trail and error and receving rewards(+/-) as feedback.
  The objective function is to maxim...
    </summary>
  

  </entry>

</feed>


